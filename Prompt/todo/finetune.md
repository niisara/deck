LLM Fine-Tuning Techniques — Full fine-tuning vs LoRA vs QLoRA vs Adapters vs Prefix Tuning vs Prompt Tuning. The "13 techniques" style would work great here.

LoRA & PEFT Methods Deep Dive — Parameter-Efficient Fine-Tuning methods: LoRA, QLoRA, IA³, DoRA, LoftQ — when to use each, rank selection, target modules, memory trade-offs.

Fine-Tuning Data Preparation — Dataset curation, deduplication, instruction formatting (Alpaca, ChatML, ShareGPT formats), data quality metrics, synthetic data generation.

Fine-Tuning Evaluation Metrics — ROUGE, BLEU, BERTScore, MT-Bench, win-rate, task-specific benchmarks, alignment evals (reward model scoring).

RLHF & Alignment Techniques — SFT → Reward Model → PPO pipeline, DPO, ORPO, RLAIF — practical cheat-sheet format.

Fine-Tuning for RAG — Domain adaptation for retrieval (embedding fine-tuning), reader fine-tuning, end-to-end RAG fine-tuning. Ties directly into your existing RAG decks.

Embedding Model Fine-Tuning — Contrastive learning, triplet loss, MNRL, hard negative mining, matryoshka embeddings. Complements your existing embedding-models-deck.

Continual Learning & Catastrophic Forgetting — EWC, replay buffers, model merging (SLERP, TIES, DARE), when fine-tuning breaks base capabilities.